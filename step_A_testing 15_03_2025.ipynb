{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Product Recognition on Store Shelves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Marco Scaramuzzi \n",
    "- Student ID: 0001057167\n",
    "- email: marco.scaramuzzi@studio.unibo.it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "Develop a computer vision system that, given a reference image for each product, is able to identify boxes of cereals of different brands from one picture of a store shelf. For each type of product displayed in the shelf the system should report:\n",
    "\n",
    "1. Number of instances.\n",
    "2. Dimension of each instance (width and height of the bounding box that enclose them in pixel)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train on model image: {0.png, 1.png, 11.png, 19.png, 24.png, 25.png, 26.png}\n",
    "#### Test on scene image: {e1.png, e2.png, e3.png, e4.png, e5.png}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required modules\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "from typing import Tuple\n",
    "from collections import defaultdict\n",
    "from glob import glob\n",
    "\n",
    "# Only for jupyter notebook visualization\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from utils import * \n",
    "from image_loading import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enviromental variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading model image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\"0.jpg\", \"1.jpg\", \"11.jpg\", \"19.jpg\", \"24.jpg\", \"25.jpg\", \"26.jpg\"]\n",
    "model_files = {name: cv2.imread(f\"models/{name}\") for name in model_names}\n",
    "\n",
    "\n",
    "# Caricamento immagini modello\n",
    "models_rgb, models_gray = load_model_images(model_files)\n",
    "\n",
    "# Separazione dei canali (R, G, B)\n",
    "red_models, green_models, blue_models = split_channels(models_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_images(models_rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing model keypoints for each channel with SIFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üü¢ 1Ô∏è‚É£ Inizializzazione di FLANN\n",
    "def initialize_flann():\n",
    "    \"\"\"\n",
    "    Inizializza il matcher FLANN con parametri predefiniti.\n",
    "\n",
    "    Returns:\n",
    "        cv2.FlannBasedMatcher: Istanza di FLANN Matcher.\n",
    "    \"\"\"\n",
    "    index_params = dict(algorithm=1, trees=5)  # FLANN_INDEX_KDTREE = 1\n",
    "    search_params = dict(checks=50)\n",
    "    return cv2.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "def extract_sift_features(image_channels):\n",
    "    \"\"\"\n",
    "    Estrae keypoints e descrittori SIFT per ogni canale dell'immagine.\n",
    "\n",
    "    Parameters:\n",
    "        image_channels (list): Lista di immagini (uno per ogni canale R, G, B).\n",
    "\n",
    "    Returns:\n",
    "        tuple: (Lista keypoints, Lista descrittori)\n",
    "    \"\"\"\n",
    "    sift = cv2.SIFT_create(contrastThreshold=0.02, edgeThreshold=10)  # üîπ Pi√π keypoints\n",
    "\n",
    "    keypoints, descriptors = [], []\n",
    "    \n",
    "    for channel in image_channels:\n",
    "        kp, des = sift.detectAndCompute(channel, None)\n",
    "        keypoints.append(kp)\n",
    "        descriptors.append(des if des is not None else np.array([]))  # üîπ Evita None\n",
    "\n",
    "    return keypoints, descriptors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estrazione feature SIFT per ciascun canale\n",
    "\n",
    "kp_model_red, des_model_red = extract_sift_features(red_models)\n",
    "kp_model_green, des_model_green = extract_sift_features(green_models)\n",
    "kp_model_blue, des_model_blue = extract_sift_features(blue_models)\n",
    "# Calcolo colori di riferimento\n",
    "# reference_colors = compute_reference_colors(models_rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_rgb, scene_gray = load_scene(\"m5\", scene_dir=\"scenes/step_B/\") # image m5.png\n",
    "\n",
    "# Separazione canali scena\n",
    "red_scene, green_scene, blue_scene = split_scene_channels(scene_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp_scene_red, des_scene_red = extract_sift_features([red_scene])\n",
    "kp_scene_green, des_scene_green = extract_sift_features([green_scene])\n",
    "kp_scene_blue, des_scene_blue = extract_sift_features([blue_scene])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inizializzazione FLANN\n",
    "flann = initialize_flann()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_keypoints(flann, model_descriptors, scene_descriptors):\n",
    "    \"\"\"\n",
    "    Esegue il matching dei keypoints tra modello e scena per ciascun canale (R, G, B).\n",
    "\n",
    "    Parameters:\n",
    "        flann (cv2.FlannBasedMatcher): Matcher FLANN.\n",
    "        model_descriptors (tuple): Tuple con descrittori dei modelli per (R, G, B).\n",
    "        scene_descriptors (tuple): Tuple con i descrittori della scena per (R, G, B).\n",
    "\n",
    "    Returns:\n",
    "        list: Lista dei match combinati per ogni modello.\n",
    "    \"\"\"\n",
    "    num_models = len(model_descriptors[0])  \n",
    "    matches = [[] for _ in range(num_models)]\n",
    "\n",
    "    for i in range(num_models):\n",
    "        matches_red = []\n",
    "        matches_green = []\n",
    "        matches_blue = []\n",
    "\n",
    "        for c, matches_channel in zip(range(3), [matches_red, matches_green, matches_blue]):\n",
    "            d_model = model_descriptors[c][i]\n",
    "            d_scene = scene_descriptors[c]\n",
    "\n",
    "            # Controlliamo che i descrittori non siano None o vuoti\n",
    "            if d_model is None or d_scene is None or len(d_model) == 0 or len(d_scene) == 0:\n",
    "                print(f\"‚ö†Ô∏è Warning: descrittori assenti per modello {i}, canale {c}. Skipping.\")\n",
    "                matches_channel.append([])\n",
    "                continue\n",
    "\n",
    "            # Conversione in float32 per OpenCV\n",
    "            d_model = np.asarray(d_model, dtype=np.float32)\n",
    "            d_scene = np.asarray(d_scene, dtype=np.float32)\n",
    "\n",
    "            # Matching con FLANN\n",
    "            matches_channel.extend(flann.knnMatch(d_model, d_scene, k=3))\n",
    "\n",
    "        matches[i] = matches_red + matches_green + matches_blue  # Unione dei match dei tre canali\n",
    "\n",
    "    return matches\n",
    "\n",
    "\n",
    "\n",
    "def extract_matched_keypoints(matches, model_keypoints, scene_keypoints):\n",
    "    \"\"\"\n",
    "    Estrae i keypoints corrispondenti dal modello e dalla scena.\n",
    "\n",
    "    Parameters:\n",
    "        matches (list): Lista dei match tra modello e scena.\n",
    "        model_keypoints (tuple): Tuple con i keypoints del modello per (R, G, B).\n",
    "        scene_keypoints (tuple): Tuple con i keypoints della scena per (R, G, B).\n",
    "\n",
    "    Returns:\n",
    "        tuple: (Keypoints del modello, Keypoints della scena)\n",
    "    \"\"\"\n",
    "    num_models = len(matches)\n",
    "    matched_model_pts = [[] for _ in range(num_models)]\n",
    "    matched_scene_pts = [[] for _ in range(num_models)]\n",
    "\n",
    "    for i in range(num_models):\n",
    "        total_matches = [m for triplet in matches[i] for m in triplet]  # Flatten dei match dei 3 canali\n",
    "        \n",
    "        # Estrazione dei keypoints del modello (prendiamo 1 match ogni 3 per evitare duplicazioni)\n",
    "        matched_model_pts[i] = np.array([(model_keypoints[0][i][match.queryIdx].pt,\n",
    "                                          model_keypoints[0][i][match.queryIdx].size)\n",
    "                                         for match in total_matches[::3]])  # Prende solo ogni 3 match\n",
    "\n",
    "        # Estrazione dei keypoints della scena (prendiamo tutti i match)\n",
    "        matched_scene_pts[i] = np.array([(scene_keypoints[match.trainIdx].pt,\n",
    "                                          scene_keypoints[match.trainIdx].size)\n",
    "                                         for match in total_matches])\n",
    "\n",
    "    return matched_model_pts, matched_scene_pts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.8.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\flann\\src\\miniflann.cpp:336: error: (-210:Unsupported format or combination of formats) in function 'cv::flann::buildIndex_'\n> type=1021\n> ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m num_models \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(des_model_red)  \u001b[38;5;66;03m# Ora il codice √® scalabile!\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# 1Ô∏è‚É£ Matching tra modello e scena\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m matches_list \u001b[38;5;241m=\u001b[39m \u001b[43mmatch_keypoints\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflann\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                               \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdes_model_red\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdes_model_green\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdes_model_blue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                               \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdes_scene_red\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdes_scene_green\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdes_scene_blue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# 2Ô∏è‚É£ Estrazione dei keypoints corrispondenti\u001b[39;00m\n\u001b[0;32m     10\u001b[0m src_pts_matches, dst_pts_matches \u001b[38;5;241m=\u001b[39m extract_matched_keypoints(matches_list, \n\u001b[0;32m     11\u001b[0m                                                              (kp_model_red, kp_model_green, kp_model_blue), \n\u001b[0;32m     12\u001b[0m                                                              (kp_scene_red, kp_scene_green, kp_scene_blue))\n",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36mmatch_keypoints\u001b[1;34m(flann, model_descriptors, scene_descriptors)\u001b[0m\n\u001b[0;32m     33\u001b[0m         d_scene \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(d_scene, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     35\u001b[0m         \u001b[38;5;66;03m# Matching con FLANN\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m         matches_channel\u001b[38;5;241m.\u001b[39mextend(\u001b[43mflann\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mknnMatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_scene\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     38\u001b[0m     matches[i] \u001b[38;5;241m=\u001b[39m matches_red \u001b[38;5;241m+\u001b[39m matches_green \u001b[38;5;241m+\u001b[39m matches_blue  \u001b[38;5;66;03m# Unione dei match dei tre canali\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m matches\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.8.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\flann\\src\\miniflann.cpp:336: error: (-210:Unsupported format or combination of formats) in function 'cv::flann::buildIndex_'\n> type=1021\n> "
     ]
    }
   ],
   "source": [
    "\n",
    "# üü¢ Esecuzione del codice\n",
    "num_models = len(des_model_red)  # Ora il codice √® scalabile!\n",
    "\n",
    "# 1Ô∏è‚É£ Matching tra modello e scena\n",
    "matches_list = match_keypoints(flann, \n",
    "                               (des_model_red, des_model_green, des_model_blue), \n",
    "                               (des_scene_red, des_scene_green, des_scene_blue))\n",
    "\n",
    "# 2Ô∏è‚É£ Estrazione dei keypoints corrispondenti\n",
    "src_pts_matches, dst_pts_matches = extract_matched_keypoints(matches_list, \n",
    "                                                             (kp_model_red, kp_model_green, kp_model_blue), \n",
    "                                                             (kp_scene_red, kp_scene_green, kp_scene_blue))\n",
    "\n",
    "# 3Ô∏è‚É£ Stampa del numero di keypoints trovati per ogni modello\n",
    "for i in range(num_models):\n",
    "    print(f\"Modello {i}: {len(src_pts_matches[i])} keypoints modello, {len(dst_pts_matches[i])} keypoints scena\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.8.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\flann\\src\\miniflann.cpp:336: error: (-210:Unsupported format or combination of formats) in function 'cv::flann::buildIndex_'\n> type=1021\n> ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)\n",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[0;32m      2\u001b[0m num_models \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(des_model_red)  \u001b[38;5;66;03m# Ora il codice √® scalabile!\u001b[39;00m\n",
      "\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# 1Ô∏è‚É£ Matching tra modello e scena\u001b[39;00m\n",
      "\u001b[1;32m----> 5\u001b[0m matches_list \u001b[38;5;241m=\u001b[39m \u001b[43mmatch_keypoints\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflann\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n",
      "\u001b[0;32m      6\u001b[0m \u001b[43m                               \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdes_model_red\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdes_model_green\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdes_model_blue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n",
      "\u001b[0;32m      7\u001b[0m \u001b[43m                               \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdes_scene_red\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdes_scene_green\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdes_scene_blue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# 2Ô∏è‚É£ Estrazione dei keypoints corrispondenti\u001b[39;00m\n",
      "\u001b[0;32m     10\u001b[0m src_pts_matches, dst_pts_matches \u001b[38;5;241m=\u001b[39m extract_matched_keypoints(matches_list, \n",
      "\u001b[0;32m     11\u001b[0m                                                              (kp_model_red, kp_model_green, kp_model_blue), \n",
      "\u001b[0;32m     12\u001b[0m                                                              (kp_scene_red, kp_scene_green, kp_scene_blue))\n",
      "\n",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36mmatch_keypoints\u001b[1;34m(flann, model_descriptors, scene_descriptors)\u001b[0m\n",
      "\u001b[0;32m     33\u001b[0m         d_scene \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(d_scene, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[0;32m     35\u001b[0m         \u001b[38;5;66;03m# Matching con FLANN\u001b[39;00m\n",
      "\u001b[1;32m---> 36\u001b[0m         matches_channel\u001b[38;5;241m.\u001b[39mextend(\u001b[43mflann\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mknnMatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_scene\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;32m     38\u001b[0m     matches[i] \u001b[38;5;241m=\u001b[39m matches_red \u001b[38;5;241m+\u001b[39m matches_green \u001b[38;5;241m+\u001b[39m matches_blue  \u001b[38;5;66;03m# Unione dei match dei tre canali\u001b[39;00m\n",
      "\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m matches\n",
      "\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.8.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\flann\\src\\miniflann.cpp:336: error: (-210:Unsupported format or combination of formats) in function 'cv::flann::buildIndex_'\n",
      "> type=1021\n",
      "> "
     ]
    }
   ],
   "source": [
    "\n",
    "# üü¢ Esecuzione del codice\n",
    "num_models = len(des_model_red)  # Ora il codice √® scalabile!\n",
    "\n",
    "# 1Ô∏è‚É£ Matching tra modello e scena\n",
    "matches_list = match_keypoints(flann, \n",
    "                               (des_model_red, des_model_green, des_model_blue), \n",
    "                               (des_scene_red, des_scene_green, des_scene_blue))\n",
    "\n",
    "# 2Ô∏è‚É£ Estrazione dei keypoints corrispondenti\n",
    "src_pts_matches, dst_pts_matches = extract_matched_keypoints(matches_list, \n",
    "                                                             (kp_model_red, kp_model_green, kp_model_blue), \n",
    "                                                             (kp_scene_red, kp_scene_green, kp_scene_blue))\n",
    "\n",
    "# 3Ô∏è‚É£ Stampa del numero di keypoints trovati per ogni modello\n",
    "for i in range(num_models):\n",
    "    print(f\"Modello {i}: {len(src_pts_matches[i])} keypoints modello, {len(dst_pts_matches[i])} keypoints scena\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GHT Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_r_table(center, source_vectors):\n",
    "    \"\"\"\n",
    "    Generates an R-Table for Generalized Hough Transform.\n",
    "    \n",
    "    Parameters:\n",
    "        center (tuple): (x, y) coordinates of the centroid.\n",
    "        source_vectors (numpy.ndarray): Array of shape (N, 2) containing keypoint positions of the models.\n",
    "\n",
    "    Returns:\n",
    "        defaultdict(list): R-Table with distances from keypoints to the centroid.\n",
    "    \"\"\"\n",
    "    r_table = defaultdict(list)\n",
    "\n",
    "    # Convert source_vectors to NumPy array for efficiency\n",
    "    source_vectors = np.array(source_vectors)\n",
    "\n",
    "    # Compute distance vectors in one vectorized operation\n",
    "    delta_x = center[0] - source_vectors[:, 0]\n",
    "    delta_y = center[1] - source_vectors[:, 1]\n",
    "\n",
    "    # Stack results efficiently\n",
    "    distances = np.column_stack((delta_x, delta_y, np.ones_like(delta_x)))  # Assume size = 1 for now\n",
    "\n",
    "    # Store each keypoint's distances in the R-table\n",
    "    for index, (dx, dy, size) in enumerate(distances):\n",
    "        r_table[index].append((dx, dy, size))\n",
    "\n",
    "    return r_table\n",
    "\n",
    "\n",
    "def accumulate_votes(r_table, shelf_image, scene_keypoints):\n",
    "    \"\"\"\n",
    "    Casts votes for barycentre position using the R-Table.\n",
    "    \n",
    "    Parameters:\n",
    "        r_table (defaultdict(list)): The R-Table storing model keypoints' vectors.\n",
    "        shelf_image (numpy.ndarray): The image of the shelf (used to get accumulator size).\n",
    "        scene_keypoints (numpy.ndarray): Array of shape (N, 2) with detected keypoints in the scene.\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: The accumulator matrix where the most voted position is likely the barycentre.\n",
    "    \"\"\"\n",
    "    # Initialize accumulator\n",
    "    accumulator = np.zeros(shelf_image.shape[:2], dtype=np.int32)\n",
    "\n",
    "    # Ensure keypoints are a NumPy array\n",
    "    scene_keypoints = np.array(scene_keypoints)\n",
    "\n",
    "    # Loop through each keypoint in the scene\n",
    "    for idx, (scene_pos, scene_size) in enumerate(scene_keypoints):\n",
    "        x_scene, y_scene = scene_pos\n",
    "\n",
    "        # Find the corresponding model keypoint index\n",
    "        model_index = idx // 3  # Assuming each model keypoint has 3 associated scene keypoints\n",
    "\n",
    "        if model_index not in r_table:\n",
    "            continue  # Skip if the model keypoint is not in the R-Table\n",
    "\n",
    "        # Get the precomputed vectors from the R-Table\n",
    "        for dx, dy, model_size in r_table[model_index]:\n",
    "            scale_ratio = scene_size / model_size  # Scale factor\n",
    "            x_accum = int(round(x_scene + scale_ratio * dx))\n",
    "            y_accum = int(round(y_scene + scale_ratio * dy))\n",
    "\n",
    "            # Ensure we don't go out of bounds\n",
    "            if 0 <= x_accum < accumulator.shape[1] and 0 <= y_accum < accumulator.shape[0]:\n",
    "                accumulator[y_accum, x_accum] += 1  # Increment vote at calculated barycentre position\n",
    "\n",
    "    return accumulator\n",
    "\n",
    "\n",
    "\n",
    "def plot_accumulator(accumulator, shelf_image):\n",
    "    \"\"\"\n",
    "    Plots the accumulator on a white image and finds barycentre candidates.\n",
    "    \n",
    "    Parameters:\n",
    "        accumulator (numpy.ndarray): The voting accumulator matrix.\n",
    "        shelf_image (numpy.ndarray): The shelf image (used to get the shape).\n",
    "    \n",
    "    Returns:\n",
    "        list: List of (x, y) barycentre candidate coordinates.\n",
    "    \"\"\"\n",
    "    # Copy shelf image and fill with white\n",
    "    height, width = accumulator.shape\n",
    "    white_image = np.full_like(shelf_image, 255)  # Faster than .copy() + .fill(255)\n",
    "    \n",
    "    # Find all candidate barycentres (votes > 2)\n",
    "    barycentres = np.argwhere(accumulator > 2)\n",
    "    \n",
    "    # Draw black points where votes > 0\n",
    "    white_image[accumulator > 0] = 0\n",
    "    \n",
    "    # Draw diagonal cross around each barycentre\n",
    "    radius = 10\n",
    "    for y, x in barycentres:\n",
    "        # Define safe bounding box using np.clip\n",
    "        y_start, y_end = np.clip([y - radius, y + radius], 0, height - 1)\n",
    "        x_start, x_end = np.clip([x - radius, x + radius], 0, width - 1)\n",
    "        \n",
    "        # Draw diagonal cross using slicing\n",
    "        white_image[y_start:y_end, x_start:x_end] = 0\n",
    "        white_image[y_start:y_end, x_end:x_start:-1] = 0  # Reverse diagonal\n",
    "        \n",
    "    # Plot the image\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.imshow(white_image, cmap='gray', vmin=0, vmax=255)\n",
    "    plt.show()\n",
    "    \n",
    "    return barycentres.tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Costruzione della R-Table e accumulatore\n",
    "r_table_list = [build_r_table((model.shape[0] // 2, model.shape[1] // 2), src_pts_matches[0][i])\n",
    "                for i, model in enumerate(red_models)]\n",
    "\n",
    "accumulator_list = [accumulate_kp(r_table_list[i], scene_gray, dst_pts_matches[0][i]) \n",
    "                    for i in range(len(red_models))]\n",
    "\n",
    "# Identificazione dei baricentri\n",
    "barycentres_list = detect_barycentres(accumulator_list, scene_gray)\n",
    "\n",
    "print(\"Identificati baricentri:\", barycentres_list)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
