{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Product Recognition on Store Shelves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Marco Scaramuzzi \n",
    "- Student ID: 0001057167\n",
    "- email: marco.scaramuzzi@studio.unibo.it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "Develop a computer vision system that, given a reference image for each product, is able to identify boxes of cereals of different brands from one picture of a store shelf. For each type of product displayed in the shelf the system should report:\n",
    "\n",
    "1. Number of instances.\n",
    "2. Dimension of each instance (width and height of the bounding box that enclose them in pixel)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train on model image: {0.png, 1.png, 11.png, 19.png, 24.png, 25.png, 26.png}\n",
    "#### Test on scene image: {e1.png, e2.png, e3.png, e4.png, e5.png}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#Import required modules\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "from typing import Tuple\n",
    "from collections import defaultdict\n",
    "from glob import glob\n",
    "\n",
    "# Only for jupyter notebook visualization\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from utils import * \n",
    "from image_loading import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading model image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_model_mapping = {\n",
    "    '0.jpg':'nes_ck_mlk',\n",
    "    '1.jpg':'chc_krv_mlk',\n",
    "    '11.jpg':'chc_krv_nuts',\n",
    "    '19.jpg':'cnt_crs_nuts',   \n",
    "    '24.jpg':'fit_pink',   \n",
    "    '25.jpg':'pops_balls',   \n",
    "    '26.jpg':'nes_duo',       \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# MODEL_NAME_TEST = '19.jpg'\n",
    "\n",
    "model_file_names = [\"0.jpg\", \"1.jpg\", \"11.jpg\", \"19.jpg\", \"24.jpg\", \"25.jpg\", \"26.jpg\"]\n",
    "model_files = {name: cv2.imread(f\"models/{name}\") for name in model_file_names}\n",
    "\n",
    "# models_rgb, model_images = load_model_images(model_files)\n",
    "\n",
    "# Caricamento immagini modello\n",
    "# img_query = load_single_model(MODEL_NAME_TEST)\n",
    "model_name = input('Insert model number: [0,1,11,19,24,25,26]')\n",
    "model_name = model_name+'.jpg'\n",
    "img_query = load_single_model(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Keypoints detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SIFT object instantion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "metadata": {},
   "outputs": [],
   "source": [
    "sift = cv2.SIFT_create(contrastThreshold=0.02, edgeThreshold=10)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using SIFT to identify the keypoints and their descriptors of a single model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describing keypoints for query and train images\n",
    "kp_query = sift.detect(img_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_images(models_rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scene"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading scene image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_index = int(input('Inserisci numero scena (1...5)'))\n",
    "img_train = load_single_scene(scene_filename=f'e{scene_index}', scene_dir=\"scenes/step_A/\") # image m5.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using SIFT to identify the keypoints and their descriptors of a single scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp_train = sift.detect(img_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Keypoints description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 820,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describing keypoints for query and train images\n",
    "kp_query, des_query = sift.compute(img_query, kp_query)\n",
    "kp_train, des_train = sift.compute(img_train, kp_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 821,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🟢 1️⃣ Inizializzazione di FLANN\n",
    "def initialize_flann():\n",
    "    \"\"\"\n",
    "    Inizializza il matcher FLANN con parametri predefiniti.\n",
    "\n",
    "    Returns:\n",
    "        cv2.FlannBasedMatcher: Istanza di FLANN Matcher.\n",
    "    \"\"\"\n",
    "    index_params = dict(algorithm=1, trees=5)  # FLANN_INDEX_KDTREE = 1\n",
    "    search_params = dict(checks=50)\n",
    "    return cv2.FlannBasedMatcher(index_params, search_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Position Estimation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def filter_false_positives_hybrid(matches: list, min_dist_threshold=250) -> list:\n",
    "    \"\"\"\n",
    "    Filtra i falsi positivi combinando il criterio di Lowe classico e quello basato sulla media (k=3).\n",
    "    \n",
    "    Accetta un match solo se:\n",
    "        - m1 è significativamente migliore di m2 (m1 < 0.7 * m2)\n",
    "        - m1 è significativamente migliore della media tra m2 e m3\n",
    "        - m1 non ha una distanza eccessiva (evita match deboli)\n",
    "\n",
    "    Parametri:\n",
    "        matches (list): Lista di triple (k=3) ottenute da knnMatch(..., k=3)\n",
    "        min_dist_threshold (float): Distanza massima per considerare un match valido.\n",
    "\n",
    "    Ritorna:\n",
    "        good (list): Lista dei match accettati.\n",
    "    \"\"\"\n",
    "    good = []\n",
    "    for m in matches:\n",
    "        if len(m) == 3:\n",
    "            m1, m2, m3 = m\n",
    "            avg_dist = (m2.distance + m3.distance) / 2.0\n",
    "            \n",
    "            if m1.distance < 0.7 * m2.distance and m1.distance < 0.75 * avg_dist and m1.distance < min_dist_threshold:\n",
    "                good.append(m1)\n",
    "    return good\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 823,
   "metadata": {},
   "outputs": [],
   "source": [
    "flann = initialize_flann()\n",
    "matches = flann.knnMatch(des_query,des_train,k=3)\n",
    "# store all the good matches as per Lowe's ratio test.\n",
    "good = filter_false_positives_hybrid(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 824,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_and_draw_homography_and_box(\n",
    "    img_query, img_train, kp_query, kp_train, good_matches,\n",
    "    model_name=\"Modello\", min_match_count=70\n",
    "):\n",
    "    \"\"\"\n",
    "    Calcola omografia e disegna solo la bounding box con nome modello nella scena.\n",
    "\n",
    "    Ritorna:\n",
    "        img_out (np.ndarray): Immagine della scena con bounding box e nome modello.\n",
    "        M (np.ndarray): Matrice di omografia calcolata (o None).\n",
    "    \"\"\"\n",
    "    if len(good_matches) > min_match_count:\n",
    "        src_pts = np.float32([kp_query[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "        dst_pts = np.float32([kp_train[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "\n",
    "        M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "\n",
    "        if M is not None:\n",
    "            h, w = img_query.shape[:2]\n",
    "            pts = np.float32([[0, 0], [0, h-1], [w-1, h-1], [w-1, 0]]).reshape(-1, 1, 2)\n",
    "            dst = cv2.perspectiveTransform(pts, M)\n",
    "\n",
    "            img_out = img_train.copy()\n",
    "            dst_pts = np.int32(dst)\n",
    "\n",
    "            # Disegna la bounding box verde\n",
    "            cv2.polylines(img_out, [dst_pts], True, (0, 255, 0), 3, cv2.LINE_AA)\n",
    "\n",
    "            # Calcolo posizione per il testo\n",
    "            top_left = tuple(dst_pts[0][0])\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            font_scale = 1.0\n",
    "            thickness = 2\n",
    "\n",
    "            # Scrive il nome modello SOPRA il rettangolo\n",
    "            text_size = cv2.getTextSize(model_name, font, font_scale, thickness)[0]\n",
    "            text_org = (top_left[0], top_left[1] - 10)\n",
    "            if text_org[1] < 0:  # se il testo finisce fuori immagine\n",
    "                text_org = (top_left[0], top_left[1] + text_size[1] + 10)\n",
    "\n",
    "            cv2.putText(img_out, model_name, text_org, font, font_scale, (0, 255, 0), thickness, cv2.LINE_AA)\n",
    "\n",
    "            # Visualizza immagine\n",
    "            plt.figure(figsize=(14, 8))\n",
    "            plt.imshow(img_out)\n",
    "            plt.axis('off')\n",
    "            plt.title(f\"Bounding Box - {model_name}\")\n",
    "            plt.show()\n",
    "\n",
    "            return img_out, M\n",
    "\n",
    "    print(f\"❌ Not enough matches are found - {len(good_matches)}/{min_match_count}\")\n",
    "    return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 825,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Not enough matches are found - 6/70\n"
     ]
    }
   ],
   "source": [
    "output_img, M = compute_and_draw_homography_and_box(\n",
    "    img_query,\n",
    "    img_train,\n",
    "    kp_query,\n",
    "    kp_train,\n",
    "    good,\n",
    "    model_name=file_to_model_mapping[model_name]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a function for object detection pipeline\n",
    "def object_retrieve(img_query, img_train, min_match_count):\n",
    "    sift = cv2.SIFT_create()\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.imshow(img_query, cmap='gray', vmin= 0, vmax=255)\n",
    "\n",
    "    kp_query = sift.detect(img_query)\n",
    "    kp_train = sift.detect(img_train)\n",
    "    kp_query, des_query = sift.compute(img_query, kp_query)\n",
    "    kp_train, des_train = sift.compute(img_train, kp_train)\n",
    "\n",
    "    FLANN_INDEX_KDTREE = 1\n",
    "    index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "    search_params = dict(checks = 50)\n",
    "    flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "    matches = flann.knnMatch(des_query,des_train,k=3)\n",
    "    # store all the good matches as per Lowe's ratio test.\n",
    "    good = filter_false_positives_hybrid(matches)\n",
    "    print(len(good))\n",
    "    if len(good)>min_match_count:\n",
    "        return len(good)\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def draw_detected_model(scene_img, bbox_dst, model_id, file_to_model_mapping, color=(0, 255, 0), thickness=3):\n",
    "    \"\"\"\n",
    "    Disegna la bounding box e il nome del modello sull'immagine della scena.\n",
    "\n",
    "    Parametri:\n",
    "        scene_img (np.ndarray): immagine di input (a colori, BGR o RGB)\n",
    "        bbox_dst (np.ndarray): bounding box trasformato (shape [4,1,2])\n",
    "        model_id (int): indice numerico del modello riconosciuto\n",
    "        file_to_model_mapping (dict): dizionario {id_modello: nome_modello}\n",
    "        color (tuple): colore RGB della bounding box e testo\n",
    "        thickness (int): spessore della bounding box\n",
    "    Ritorna:\n",
    "        scene_img (np.ndarray): immagine con annotazioni\n",
    "    \"\"\"\n",
    "    img_copy = scene_img.copy()\n",
    "\n",
    "    # Disegna la bounding box\n",
    "    pts = np.int32(bbox_dst)\n",
    "    img_copy = cv2.polylines(img_copy, [pts], isClosed=True, color=color, thickness=thickness, lineType=cv2.LINE_AA)\n",
    "\n",
    "    # Recupera il nome del modello\n",
    "    model_name = file_to_model_mapping.get(model_id, f\"Modello {model_id}\")\n",
    "\n",
    "    # Calcola dimensioni del testo\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = 1.0\n",
    "    text_thickness = 3\n",
    "    (text_width, text_height), _ = cv2.getTextSize(model_name, font, font_scale, text_thickness)\n",
    "\n",
    "    # Posizionamento del testo sotto l'angolo in basso a sinistra della box\n",
    "    bottom_left = (int(pts[1][0][0]), int(pts[1][0][1]) + 10)\n",
    "    top_left = (int(pts[1][0][0]), int(pts[1][0][1]) - text_height - 5)\n",
    "\n",
    "    # Sfondo del testo (rettangolo)\n",
    "    cv2.rectangle(img_copy, bottom_left, (bottom_left[0] + text_width, bottom_left[1] - text_height - 5), color, cv2.FILLED)\n",
    "\n",
    "    # Scrivi il testo\n",
    "    cv2.putText(img_copy, model_name, (bottom_left[0], bottom_left[1] - 5), font, font_scale, (0, 0, 0), text_thickness, cv2.LINE_AA)\n",
    "\n",
    "    return img_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trova tutti i file immagine nella cartella 'scenes/step_B'\n",
    "from glob import glob\n",
    "imgs_train = glob('scenes/step_A/*.png')\n",
    "\n",
    "\n",
    "def compare_model_to_scenes(step='step_A', img_ext='.png'):\n",
    "    imgs_train = glob(f'scenes/{step}/*{img_ext}')\n",
    "\n",
    "    # Iterating among all images and looking for the query object.\n",
    "    for path in imgs_train:\n",
    "        img_train_bgr = cv2.imread(path)  # <-- immagine a colori in BGR\n",
    "        img_train = cv2.cvtColor(img_train_bgr, cv2.COLOR_BGR2RGB)  # converti in RGB\n",
    "\n",
    "        found = object_retrieve(img_query, img_train, min_match_count=70)\n",
    "        print(\"Found query object in {}:{}\".format(path,found > 0))\n",
    "        if found > 0:\n",
    "            plt.figure(figsize=(15,6))\n",
    "            plt.imshow(img_train,   vmin= 0, vmax=255)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare_model_to_scenes(step='step_A')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
